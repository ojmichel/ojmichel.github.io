<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Oscar Michel's Webpage</title>
    <link rel="stylesheet" type="text/css" href="css/style.css">

</head>
<body>
        <div class="box-1">
            <div style="margin-bottom: 50px;">
                <img src="uchicago.png" width="10%" style="float: right;">
                <img src="ai2.png" width="10%" style="float: right;">
                <img src="nyu.png" width="7%" style="float: right;">
                <h1>Oscar Michel's Webpage</h1>
            </div>


            <div class="box-2">
                <div class="box-2-element" style="padding-top: 5%;"><img src="me4.jpeg" class="image"></div>
                    <div class="box-2-element" style="flex: 2;">
                        <h2>About Me</h2>
                        <p>I am first year PhD student at NYU Courant advised by Prof. Saining Xie. My research is in computer vision and AI.</p>
                        <p>Previously, I was a predoctoral researcher at the Allen Institute for AI working primarily with Dr. Tanmay Gupta and Dr. Ani Kembhavi. Before that I was an undergraduate at the University of Chicago, where I received a degree in mathematics. There I was fortunate to work with Prof. Rana Hanocka and Prof. Michael Maire.</p>
                        
                    </div>
                <div class="box-2-element">
                    <h2> Contact </h2>
                        <ul>
                            <li>
                                <a class="contact" href="mailto:micheloscar20@gmail.com">Email</a>
                            </li>
                            <li>
                                <a class="contact" href="https://scholar.google.com/citations?user=zczXAk8AAAAJ&hl=en&oi=ao">Google Scholar</a>
                            </li>
                            <li>
                                <a class="contact" href="https://github.com/ojmichel">GitHub</a>
                            </li>
                            <li>
                                <a class="contact" href="https://www.linkedin.com/in/oscar-michel-82a162145/">LinkedIn</a>
                            </li>
                        </ul>


                </div>

            </div>
                <h1>
                    Publications & Preprints
                </h1>
                <table>
                    <tr></tr>
                        <td><img src="mmolmo.png" class="image-research"></td>
                        <td>
                            <div class="research-text">
                                <p>
                                    <b><i>Molmo and PixMo</i>: Open Weights and Open Data for State-of-the-Art Multimodal Models</b><br>
                                    Matt Deitke, Chirstopher Clark, et al.<br>
                                    Technical report 2024 <br><br>
                                    A family of open multimodal models that are as good as the best commerical ones. My contribution was creating a novel evaluation benchmark for testing fine-grained world knowledge in VQA.  
                                </p>
                                <ul>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://arxiv.org/abs/2406.11775">Arxiv<a>
                                    </li >
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://www.task-me-anything.org/">Page</a>
                                    </li>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://github.com/JieyuZ2/TaskMeAnything">Code</a>
                                    </li>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://scholar.googleusercontent.com/scholar.bib?q=info:JZufGoZ2GgsJ:scholar.google.com/&output=citation&scisdr=ClHSHeTeEIu4qkOvmzU:AFWwaeYAAAAAZvSpgzV5emuHp3RVxCxWeJKv6Us&scisig=AFWwaeYAAAAAZvSpg07AZoK4EUSDEpTfAtEyr7I&scisf=4&ct=citation&cd=-1&hl=en">BibTex</a>
                                    </li>
                                </ul>
                            </div>
                        </td>
                    <tr>
                        <td><img src="tma.png" class="image-research"></td>
                        <td>
                            <div class="research-text">
                                <p>
                                    <b>Task Me Anything</b><br>
                                    Jieyu Zhang, Weikai Huang, Zixian Ma, Oscar Michel, Dong He, Tanmay Gupta, Wei-Chu Ma, Ali Farhadi, Aniruddha Kembhavi, Ranjay Krishna<br>
                                    Arxiv 2024 <br><br>
                                    A new way of thinking about how we can evaluate models using synthetic visual data.
                                </p>
                                <ul>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://molmo.allenai.org/paper.pdf">Paper<a>
                                    </li >
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://molmo.allenai.org/blog">Page</a>
                                    </li>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://huggingface.co/collections/allenai/molmo-66f379e6fe3b8ef090a8ca19">Models</a>
                                    </li>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://molmo.allenai.org/">Demo</a>
                                    </li>
                                </ul>
                            </div>
                        </td>
                    </tr>
                    <tr>
                        <td><img src="object_edit.png" class="image-research"></td>
                        <td>
                            <div class="research-text">
                                <p>
                                    <b>OBJect 3DIT: Language-guided 3D-aware Image Editing</b><br>
                                    Oscar Michel, Anand Bhattad, Eli VanderBilt, Ranjay Krishna, Ani Kembhavi, Tanmay Gupta<br>
                                    NeurIPS 2023 <br><br>
                                   A synthetic dataset and a model that learns to rotate, translate, insert and remove objects identified by language in a scene. It can transfer to real world images.
                                </p>
                                <ul>
                                    <li class="paper-li">
                                        <a class="paper-link" href="http://arxiv.org/abs/2307.11073">Arxiv<a>
                                    </li >
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://prior.allenai.org/projects/object-edit">Page</a>
                                    </li>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://github.com/allenai/object-edit">Code</a>
                                    </li>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://scholar.googleusercontent.com/scholar.bib?q=info:g0nKc6oHGMYJ:scholar.google.com/&output=citation&scisdr=ClHSHeTeEIu4qkOvheg:AFWwaeYAAAAAZvSpnej4slcrbqi_FYEnR0Xpcmk&scisig=AFWwaeYAAAAAZvSpnXAuqK0f37xx7JCinB8dPEQ&scisf=4&ct=citation&cd=-1&hl=en">BibTex</a>
                                    </li>
                                </ul>
                            </div>
                        </td>
                    </tr>
                    <tr>
                        <td><img src="objaverse_xl.png" class="image-research"></td>
                        <td>
                            <div class="research-text">
                                <p>
                                    <b>Objaverse-XL: A Universe of 10M+ 3D Objects
                                    </b><br>
                                    Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadr, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, Ali Farhadi<br>
                                    NeurIPS Datasets & Benchmarks 2023 <br><br>
                                   A dataset 10x the scale of Objaverse 1.0 and a 3D object reconstruction foundation model.
                                </p>
                                <ul>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://arxiv.org/abs/2307.05663">Arxiv<a>
                                    </li >
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://objaverse.allenai.org/">Page</a>
                                    </li>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://scholar.googleusercontent.com/scholar.bib?q=info:IjK6rHxSo_kJ:scholar.google.com/&output=citation&scisdr=ChUnITo9EN6N_iIN3-c:ABFrs3wAAAAAZLoLx-e_eUd7skELaOe5L-GpNbQ&scisig=ABFrs3wAAAAAZLoLx92vaXqEDLg9fnmdFyz6BMk&scisf=4&ct=citation&cd=-1&hl=en">BibTex</a>
                                    </li>
                                </ul>
                            </div>
                        </td>
                    </tr>
                    <tr>
                        <td><img src="objaverse.png" class="image-research"></td>
                        <td>
                            <div class="research-text">
                                <p>
                                    <b>Objaverse: A Universe of Annotated 3D Objects</b><br>
                                    Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, Ali Farhadi<br>
                                    CVPR 2023 <br><br>
                                   Objaverse is a large dataset of 800k+ 3D models.
                                </p>
                                <ul>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://arxiv.org/abs/2212.08051">Arxiv<a>
                                    </li >
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://objaverse.allenai.org/">Page</a>
                                    </li>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://colab.research.google.com/drive/1ZLA4QufsiI_RuNlamKqV7D7mn40FbWoY?usp=sharing">Colab</a>
                                    </li>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://scholar.googleusercontent.com/scholar.bib?q=info:TqZ8T8ei7JQJ:scholar.google.com/&output=citation&scisdr=CgXN7Cs9EN6N-Wx0GoI:AAGBfm0AAAAAY_RyAoITO0fJwhVvTPqnoc11lhGSdVwv&scisig=AAGBfm0AAAAAY_RyAiCmfjt2mxCN_wOn_n2TmJAmXAdC&scisf=4&ct=citation&cd=-1&hl=en">BibTex</a>
                                    </li>
                                </ul>
                            </div>
                        </td>
                    </tr>
                    <tr>
                        <td><img src="t2m.png" class="image-research"></td>
                        <td>
                            <div class="research-text">
                                <p>
                                    <b>Text2Mesh: Text-Driven Neural Stylization for Meshes</b><br>
                                    Oscar Michel<sup>*</sup>, Roi Bar On<sup>*</sup>, Richard Liu<sup>*</sup>, Sagie Benaim, Rana Hanocka<br>
                                    CVPR 2022 <b>(Oral)</b><br><br>
                                    Text2Mesh is an algorithm for language guided stylization of a 3D object.
                                </p>
                                <ul>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://arxiv.org/abs/2112.03221">Arxiv<a>
                                    </li >
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://threedle.github.io/text2mesh/">Page</a>
                                    </li>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://github.com/threedle/text2mesh/">Code</a>
                                    </li>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://scholar.googleusercontent.com/scholar.bib?q=info:qsePEd68T1YJ:scholar.google.com/&output=citation&scisdr=CgXjsOABEN2p7phwoN8:AAGBfm0AAAAAYhR2uN91QNB0MQiq2Hlns40lBaWi3PI0&scisig=AAGBfm0AAAAAYhR2uEAX3G0Fa_aOc4wRkuk-9b5y4Ieo&scisf=4&ct=citation&cd=-1&hl=en">BibTex</a>
                                    </li>
                                </ul>
        
                            </div>
                        </td>
                    </tr>
                </table>
                <!-- <div class="research-box">
                    <div style="aspect-ratio: 1;"><img src="t2m.png" class="image-research"></div>
                    <div class="research-text">
                        <p>
                            <b>Objaverse: A Universe of Annotated 3D Objects</b><br>
                            Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwid Schmidt, Kiana Ehsani, Aniruddha Kembhavi, Ali Farhadi
                            Arxiv 2022 <br><br>
                           Objaverse is a large dataset of 800k+ 3D models.
                        </p>
                        <ul>
                            <li class="paper-li">
                                <a class="paper-link" href="https://arxiv.org/abs/2112.03221">Arxiv<a>
                            </li >
                            <li class="paper-li">
                                <a class="paper-link" href="https://threedle.github.io/text2mesh/">Page</a>
                            </li>
                            <li class="paper-li">
                                <a class="paper-link" href="https://github.com/threedle/text2mesh/">Code</a>
                            </li>
                            <li class="paper-li">
                                <a class="paper-link" href="https://scholar.googleusercontent.com/scholar.bib?q=info:qsePEd68T1YJ:scholar.google.com/&output=citation&scisdr=CgXjsOABEN2p7phwoN8:AAGBfm0AAAAAYhR2uN91QNB0MQiq2Hlns40lBaWi3PI0&scisig=AAGBfm0AAAAAYhR2uEAX3G0Fa_aOc4wRkuk-9b5y4Ieo&scisf=4&ct=citation&cd=-1&hl=en">BibTex</a>
                            </li>
                        </ul>

                    </div>
                </div>
                <div class="research-box">
                    <div style="aspect-ratio: 1;"><img src="t2m.png" class="image-research"></div>
                    <div class="research-text">
                        <p>
                            <b>Text2Mesh: Text-Driven Neural Stylization for Meshes</b><br>
                            Oscar Michel<sup>*</sup>, Roi Bar On<sup>*</sup>, Richard Liu<sup>*</sup>, Sagie Benaim, Rana Hanocka<br>
                            CVPR 2022 <b>(Oral)</b><br><br>
                            We achieve language driven stylization of a 3D object by encoding color and geometric displacements over the objectâ€™s surface with a neural network. A differential renderer enables training the network to produce a style that has similar CLIP embeddings to the text prompt.
                        </p>
                        <ul>
                            <li class="paper-li">
                                <a class="paper-link" href="https://arxiv.org/abs/2112.03221">Arxiv<a>
                            </li >
                            <li class="paper-li">
                                <a class="paper-link" href="https://threedle.github.io/text2mesh/">Page</a>
                            </li>
                            <li class="paper-li">
                                <a class="paper-link" href="https://github.com/threedle/text2mesh/">Code</a>
                            </li>
                            <li class="paper-li">
                                <a class="paper-link" href="https://scholar.googleusercontent.com/scholar.bib?q=info:qsePEd68T1YJ:scholar.google.com/&output=citation&scisdr=CgXjsOABEN2p7phwoN8:AAGBfm0AAAAAYhR2uN91QNB0MQiq2Hlns40lBaWi3PI0&scisig=AAGBfm0AAAAAYhR2uEAX3G0Fa_aOc4wRkuk-9b5y4Ieo&scisf=4&ct=citation&cd=-1&hl=en">BibTex</a>
                            </li>
                        </ul>

                    </div>
                </div> -->


        </div>
        
</body>
</html>
