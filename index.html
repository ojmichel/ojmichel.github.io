<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Oscar Michel's Webpage</title>
    <link rel="stylesheet" type="text/css" href="css/style.css">

</head>
<body>
        <div class="box-1">
            <div style="margin-bottom: 50px;">
                <img src="uchicago.png" width="10%" style="float: right;">
                <img src="ai2.png" width="10%" style="float: right;">
                <h1>Oscar Michel's Webpage</h1>
            </div>


            <div class="box-2">
                <div class="box-2-element"><img src="me.jpg" class="image"></div>
                    <div class="box-2-element">
                        <h2>About Me</h2>
                        <p>I am a Predoctoral Young Investigator at the Allen Institute for AI, working in PRIOR, the computer vision team. My research interests include multimodal and 3D computer vision.</p>
                        <p>Previously, I was an undergraduate at the University of Chicago, where I received a degree in mathematics. I was fortunate to work with Professor Rana Hanocka and Professor Michael Maire.</p>
                        
                    </div>
                <div class="box-2-element">
                    <h2> Contact </h2>
                        <ul>
                            <li>
                                <a class="contact" href="mailto:micheloscar20@gmail.com">Email</a>
                            </li>
                            <li>
                                <a class="contact" href="https://scholar.google.com/citations?user=zczXAk8AAAAJ&hl=en&oi=ao">Google Scholar</a>
                            </li>
                            <li>
                                <a class="contact" href="https://github.com/ojmichel">GitHub</a>
                            </li>
                            <li>
                                <a class="contact" href="https://www.linkedin.com/in/oscar-michel-82a162145/">LinkedIn</a>
                            </li>
                        </ul>


                </div>

            </div>
                <h1>
                    Research
                </h1>
                <table>
                    <tr>
                        <td><img src="object_edit.png" class="image-research"></td>
                        <td>
                            <div class="research-text">
                                <p>
                                    <b>OBJect 3DIT: Language-guided 3D-aware Image Editing</b><br>
                                    Oscar Michel, Anand Bhattad, Eli VanderBilt, Ranjay Krishna, Ani Kembhavi, Tanmay Gupta<br>
                                    NeurIPS 2023 <br><br>
                                   A synthetic dataset and a model that learns to rotate, translate, insert and remove objects identified by language in a scene. It can transfer to real world images.
                                </p>
                                <ul>
                                    <li class="paper-li">
                                        <a class="paper-link" href="http://arxiv.org/abs/2307.11073">Arxiv<a>
                                    </li >
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://prior.allenai.org/projects/object-edit">Page</a>
                                    </li>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://ojmichel.github.io/">BibTex</a>
                                    </li>
                                </ul>
                            </div>
                        </td>
                    </tr>
                    <tr>
                        <td><img src="objaverse_xl.png" class="image-research"></td>
                        <td>
                            <div class="research-text">
                                <p>
                                    <b>Objaverse-XL: A Universe of 10M+ 3D Objects
                                    </b><br>
                                    Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadr, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, Ali Farhadi<br>
                                    NeurIPS Datasets & Benchmarks 2023 <br><br>
                                   An even more massive dataset of 3D objects and a state of the art model for rotating objects.
                                </p>
                                <ul>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://arxiv.org/abs/2307.05663">Arxiv<a>
                                    </li >
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://objaverse.allenai.org/">Page</a>
                                    </li>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://scholar.googleusercontent.com/scholar.bib?q=info:IjK6rHxSo_kJ:scholar.google.com/&output=citation&scisdr=ChUnITo9EN6N_iIN3-c:ABFrs3wAAAAAZLoLx-e_eUd7skELaOe5L-GpNbQ&scisig=ABFrs3wAAAAAZLoLx92vaXqEDLg9fnmdFyz6BMk&scisf=4&ct=citation&cd=-1&hl=en">BibTex</a>
                                    </li>
                                </ul>
                            </div>
                        </td>
                    </tr>
                    <tr>
                        <td><img src="objaverse.png" class="image-research"></td>
                        <td>
                            <div class="research-text">
                                <p>
                                    <b>Objaverse: A Universe of Annotated 3D Objects</b><br>
                                    Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, Ali Farhadi<br>
                                    CVPR 2023 <br><br>
                                   Objaverse is a large dataset of 800k+ 3D models.
                                </p>
                                <ul>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://arxiv.org/abs/2212.08051">Arxiv<a>
                                    </li >
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://objaverse.allenai.org/">Page</a>
                                    </li>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://colab.research.google.com/drive/1ZLA4QufsiI_RuNlamKqV7D7mn40FbWoY?usp=sharing">Colab</a>
                                    </li>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://scholar.googleusercontent.com/scholar.bib?q=info:TqZ8T8ei7JQJ:scholar.google.com/&output=citation&scisdr=CgXN7Cs9EN6N-Wx0GoI:AAGBfm0AAAAAY_RyAoITO0fJwhVvTPqnoc11lhGSdVwv&scisig=AAGBfm0AAAAAY_RyAiCmfjt2mxCN_wOn_n2TmJAmXAdC&scisf=4&ct=citation&cd=-1&hl=en">BibTex</a>
                                    </li>
                                </ul>
                            </div>
                        </td>
                    </tr>
                    <tr>
                        <td><img src="t2m.png" class="image-research"></td>
                        <td>
                            <div class="research-text">
                                <p>
                                    <b>Text2Mesh: Text-Driven Neural Stylization for Meshes</b><br>
                                    Oscar Michel<sup>*</sup>, Roi Bar On<sup>*</sup>, Richard Liu<sup>*</sup>, Sagie Benaim, Rana Hanocka<br>
                                    CVPR 2022 <b>(Oral)</b><br><br>
                                    Text2Mesh is an algorithm for language guided stylization of a 3D object.
                                </p>
                                <ul>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://arxiv.org/abs/2112.03221">Arxiv<a>
                                    </li >
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://threedle.github.io/text2mesh/">Page</a>
                                    </li>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://github.com/threedle/text2mesh/">Code</a>
                                    </li>
                                    <li class="paper-li">
                                        <a class="paper-link" href="https://scholar.googleusercontent.com/scholar.bib?q=info:qsePEd68T1YJ:scholar.google.com/&output=citation&scisdr=CgXjsOABEN2p7phwoN8:AAGBfm0AAAAAYhR2uN91QNB0MQiq2Hlns40lBaWi3PI0&scisig=AAGBfm0AAAAAYhR2uEAX3G0Fa_aOc4wRkuk-9b5y4Ieo&scisf=4&ct=citation&cd=-1&hl=en">BibTex</a>
                                    </li>
                                </ul>
        
                            </div>
                        </td>
                    </tr>
                </table>
                <!-- <div class="research-box">
                    <div style="aspect-ratio: 1;"><img src="t2m.png" class="image-research"></div>
                    <div class="research-text">
                        <p>
                            <b>Objaverse: A Universe of Annotated 3D Objects</b><br>
                            Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwid Schmidt, Kiana Ehsani, Aniruddha Kembhavi, Ali Farhadi
                            Arxiv 2022 <br><br>
                           Objaverse is a large dataset of 800k+ 3D models.
                        </p>
                        <ul>
                            <li class="paper-li">
                                <a class="paper-link" href="https://arxiv.org/abs/2112.03221">Arxiv<a>
                            </li >
                            <li class="paper-li">
                                <a class="paper-link" href="https://threedle.github.io/text2mesh/">Page</a>
                            </li>
                            <li class="paper-li">
                                <a class="paper-link" href="https://github.com/threedle/text2mesh/">Code</a>
                            </li>
                            <li class="paper-li">
                                <a class="paper-link" href="https://scholar.googleusercontent.com/scholar.bib?q=info:qsePEd68T1YJ:scholar.google.com/&output=citation&scisdr=CgXjsOABEN2p7phwoN8:AAGBfm0AAAAAYhR2uN91QNB0MQiq2Hlns40lBaWi3PI0&scisig=AAGBfm0AAAAAYhR2uEAX3G0Fa_aOc4wRkuk-9b5y4Ieo&scisf=4&ct=citation&cd=-1&hl=en">BibTex</a>
                            </li>
                        </ul>

                    </div>
                </div>
                <div class="research-box">
                    <div style="aspect-ratio: 1;"><img src="t2m.png" class="image-research"></div>
                    <div class="research-text">
                        <p>
                            <b>Text2Mesh: Text-Driven Neural Stylization for Meshes</b><br>
                            Oscar Michel<sup>*</sup>, Roi Bar On<sup>*</sup>, Richard Liu<sup>*</sup>, Sagie Benaim, Rana Hanocka<br>
                            CVPR 2022 <b>(Oral)</b><br><br>
                            We achieve language driven stylization of a 3D object by encoding color and geometric displacements over the objectâ€™s surface with a neural network. A differential renderer enables training the network to produce a style that has similar CLIP embeddings to the text prompt.
                        </p>
                        <ul>
                            <li class="paper-li">
                                <a class="paper-link" href="https://arxiv.org/abs/2112.03221">Arxiv<a>
                            </li >
                            <li class="paper-li">
                                <a class="paper-link" href="https://threedle.github.io/text2mesh/">Page</a>
                            </li>
                            <li class="paper-li">
                                <a class="paper-link" href="https://github.com/threedle/text2mesh/">Code</a>
                            </li>
                            <li class="paper-li">
                                <a class="paper-link" href="https://scholar.googleusercontent.com/scholar.bib?q=info:qsePEd68T1YJ:scholar.google.com/&output=citation&scisdr=CgXjsOABEN2p7phwoN8:AAGBfm0AAAAAYhR2uN91QNB0MQiq2Hlns40lBaWi3PI0&scisig=AAGBfm0AAAAAYhR2uEAX3G0Fa_aOc4wRkuk-9b5y4Ieo&scisf=4&ct=citation&cd=-1&hl=en">BibTex</a>
                            </li>
                        </ul>

                    </div>
                </div> -->


        </div>
        
</body>
</html>
